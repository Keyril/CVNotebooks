{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "847eff4b6db4fdfd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import math"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T07:06:40.932041800Z",
     "start_time": "2024-01-07T07:06:40.900042400Z"
    }
   },
   "id": "31cd03a5dc0ad304",
   "execution_count": 283
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def draw_matches(image1, image2, kp1, kp2, matches):\n",
    "    img_matches = cv2.drawMatches(image1, kp1, image2, kp2, matches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "    # Display the matches\n",
    "    cv2.imshow(\"Sift_Matches\", img_matches)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T07:06:40.933041800Z",
     "start_time": "2024-01-07T07:06:40.911042200Z"
    }
   },
   "id": "7b276dc2f303db59",
   "execution_count": 284
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_homography_matrix(kp1, kp2, matches):\n",
    "    \"\"\" \n",
    "        Takes the matches and the keypoints for two images and computes a homography matrix.\n",
    "        Written for the purpose of using inside a RANSAC algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    h = []\n",
    "\n",
    "    for ind in range(len(matches)):\n",
    "        p = kp1[matches[ind].queryIdx].pt\n",
    "        q = kp2[matches[ind].trainIdx].pt\n",
    "        x, y = p\n",
    "        _x, _y = q\n",
    "        h_part1, h_part2 = [x, y, 1, 0, 0, 0, -_x*x, -_x*y, -_x], [0, 0, 0, x, y, 1, -_y*x, -_y*y, -_y]\n",
    "        h.append(h_part1)\n",
    "        h.append(h_part2)\n",
    "\n",
    "    _, _, V = np.linalg.svd(h)\n",
    "    h = V[-1].reshape(3, 3)\n",
    "    \n",
    "    return h"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T07:06:40.944044Z",
     "start_time": "2024-01-07T07:06:40.928041500Z"
    }
   },
   "id": "bd2ce2fc83969fb9",
   "execution_count": 285
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def check_ransac_error(kp1, kp2, matches, h, threshold):\n",
    "\n",
    "    # inline functions for estimating points in image 2 from image 1 using the homography matrix\n",
    "    x_prime = lambda x, y, h1: (h1[0][0] * x + h1[0][1] * y + h1[0][2]) / (h1[2][0] * x + h1[2][1] * y + h1[2][2])\n",
    "    y_prime = lambda x, y, h1: (h1[1][0] * x + h1[1][1] * y + h1[1][2]) / (h1[2][0] * x + h1[2][1] * y + h1[2][2])\n",
    "    \n",
    "    inliers = 0\n",
    "    for ind in range(len(matches)):\n",
    "        x1, y1 = kp1[matches[ind].queryIdx].pt\n",
    "        x2, y2 = kp2[matches[ind].trainIdx].pt\n",
    "        \n",
    "        _x, _y = x_prime(x1, y1, h), y_prime(x1, y1, h)\n",
    "\n",
    "        distance = math.sqrt(math.pow((x2 - _x), 2) + math.pow((y2 - _y), 2))\n",
    "        if distance < threshold:\n",
    "            inliers += 1\n",
    "    \n",
    "    return inliers"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T07:06:40.963041100Z",
     "start_time": "2024-01-07T07:06:40.945041900Z"
    }
   },
   "id": "afd267c504c8974a",
   "execution_count": 286
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def ransac(matches, kp1, kp2, iterations=100, threshold=5.0):\n",
    "    \"\"\"\n",
    "    :param matches: total list of matches between two images\n",
    "    :param kp1: keypoints from image 1\n",
    "    :param kp2: keypoints from image 2\n",
    "    :param iterations: number of times to run homography estimation on subset of matches\n",
    "    :param threshold: distance threshold for considering a point as an inlier instead of an outlier\n",
    "    :return: best homography matrix computed from the estimation process\n",
    "    \"\"\"\n",
    "    \n",
    "    best_inliers = 0\n",
    "    best_homography = None\n",
    "    best_acc = None\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        # Randomly select 4 matches\n",
    "        random_matches = np.random.choice(matches, 4, replace=False)\n",
    "        \n",
    "        # compute homography\n",
    "        h = get_homography_matrix(kp1, kp2, random_matches)\n",
    "        # compute error rate for this homography\n",
    "        error = check_ransac_error(kp1, kp2, matches, h, threshold)\n",
    "        if error > best_inliers:\n",
    "            best_acc = error / len(matches)\n",
    "            best_homography = h\n",
    "            best_inliers = error\n",
    "    \n",
    "    print(\"best accuracy rate\", best_acc)\n",
    "    print(\"# of inliers:\", best_inliers, \"/\", len(matches))\n",
    "    \n",
    "    return best_homography"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T07:06:40.973043800Z",
     "start_time": "2024-01-07T07:06:40.960043300Z"
    }
   },
   "id": "e28e0bfdfbf9e4c6",
   "execution_count": 287
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def stitch_images(image1_path, image2_path):\n",
    "    # Load the images\n",
    "    img1 = cv2.imread(image1_path, cv2.COLOR_BGR2GRAY)\n",
    "    img2 = cv2.imread(image2_path, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Initialize SIFT\n",
    "    sift = cv2.SIFT_create()\n",
    "    \n",
    "    # Detect keypoints and descriptors\n",
    "    kp1, desc1 = sift.detectAndCompute(img1, None)\n",
    "    kp2, desc2 = sift.detectAndCompute(img2, None)\n",
    "    \n",
    "    ''' \n",
    "    using FLANN over KNN to find matches because it is faster at the cost cost of accuracy\n",
    "    in comparison (as KNN is exhaustive). This is because image stitching does not need 100% accuracy\n",
    "    and the tradeoff for computation speed is better for scalability. Ex: multiple 4k+ images\n",
    "    '''\n",
    "    \n",
    "    # The index for KDTREE in FLANN\n",
    "    FLANN_INDEX_KDTREE = 1\n",
    "    # Initialize index params\n",
    "    index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "    # Initialize search params\n",
    "    search_params = dict(checks=50)\n",
    "    \n",
    "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "    \n",
    "    # look for matches going from the image to the right -> left (not entirely sure why but the results is a lot more accurate)\n",
    "    matches = flann.knnMatch(desc2, desc1, k=2)\n",
    "\n",
    "    # Apply ratio test to filter good matches\n",
    "    threshold=0.7\n",
    "    good_matches = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < threshold * n.distance:\n",
    "            good_matches.append(m)\n",
    "\n",
    "    # uncomment if you want to see matches visualized on the two images\n",
    "    # draw_matches(img1, img2, kp2, kp1, good_matches)\n",
    "    \n",
    "    # Find Homography using RANSAC\n",
    "    h = ransac(good_matches, kp2, kp1, 1000, 5.0)\n",
    "    \n",
    "    # Warp Image to so that you can stitch images together accurately\n",
    "    panorama = cv2.warpPerspective(img2, h, ((img2.shape[1] + img1.shape[1]), img1.shape[0]))\n",
    "    \n",
    "    # Attach the first image on top of the warped image so that they build a nice panorama\n",
    "    panorama[0:img1.shape[0], 0:img1.shape[1]] = img1\n",
    "\n",
    "    return panorama\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T07:06:40.990043800Z",
     "start_time": "2024-01-07T07:06:40.973043800Z"
    }
   },
   "id": "79a39444399dfdd9",
   "execution_count": 288
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best accuracy rate 0.9555555555555556\n",
      "# of inliers: 215 / 225\n"
     ]
    }
   ],
   "source": [
    "img1_path = 'landscape_1.jpg'\n",
    "img2_path = 'landscape_2.jpg'\n",
    "stitched_image = stitch_images(img1_path, img2_path)\n",
    "cv2.imshow('Stitched Image', stitched_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T07:06:44.102935900Z",
     "start_time": "2024-01-07T07:06:40.990043800Z"
    }
   },
   "id": "3e663bb5434a0e84",
   "execution_count": 289
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
