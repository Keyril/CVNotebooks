{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function for reading an image from a path and returning its features (keypoints and descriptors)\n",
    "def extract_features(image_path):\n",
    "\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    sift = cv2.SIFT_create()\n",
    "    keypoints, descriptors = sift.detectAndCompute(img, None)\n",
    "\n",
    "    return keypoints, descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_features(descriptors_list):\n",
    "    # Brute force matcher\n",
    "    bf = cv2.BFMatcher()\n",
    "\n",
    "    # Dictionary to store matches between all pairs of images\n",
    "    matches_dict = {}\n",
    "\n",
    "    # Iterate over all pairs of images\n",
    "    for i in range(len(descriptors_list)):\n",
    "        print(\"Looping through image: \", i)\n",
    "        for j in range(len(descriptors_list)):\n",
    "            # skip if the image is itself\n",
    "            if i == j: continue\n",
    "            # Match descriptors between image i and image j\n",
    "            matches = bf.knnMatch(descriptors_list[i], descriptors_list[j], k=2)\n",
    "            # only consider good matches\n",
    "            threshold=0.5\n",
    "            good_matches = []\n",
    "            for m, n in matches:\n",
    "                if m.distance < threshold * n.distance:\n",
    "                    good_matches.append(m)\n",
    "            matches_dict[(i, j)] = good_matches\n",
    "\n",
    "    return matches_dict\n",
    "\n",
    "# this function is for getting all the matches with a single image as the source (only used for testing)\n",
    "def single_image_match_features(descriptors_list, i=0):\n",
    "    bf = cv2.BFMatcher()\n",
    "    matches_dict = {}\n",
    "\n",
    "    for j in range(len(descriptors_list)):\n",
    "        if i == j: continue\n",
    "        # Match descriptors between a single image j and image i\n",
    "        matches = bf.knnMatch(descriptors_list[i], descriptors_list[j], k=2)\n",
    "\n",
    "        threshold=0.7\n",
    "        good_matches = []\n",
    "        for m, n in matches:\n",
    "            if m.distance < threshold * n.distance:\n",
    "                good_matches.append(m)\n",
    "        matches_dict[(i, j)] = good_matches\n",
    "\n",
    "    return matches_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of image paths\n",
    "image_paths = [f for f in os.listdir('assets/airpods/test/') if '.JPEG' in f]\n",
    "\n",
    "# Extracter features and descriptors for the images and store them in lists using the image index\n",
    "descriptors_list = []\n",
    "keypoints_list = []\n",
    "for image_path in image_paths:\n",
    "    keypoints, descriptors = extract_features('assets/airpods/test/' + image_path)\n",
    "    descriptors_list.append(descriptors)\n",
    "    keypoints_list.append(keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looping through image:  0\n",
      "Looping through image:  1\n",
      "Looping through image:  2\n",
      "Looping through image:  3\n",
      "Looping through image:  4\n",
      "Looping through image:  5\n",
      "Looping through image:  6\n",
      "Looping through image:  7\n",
      "Looping through image:  8\n",
      "Looping through image:  9\n",
      "Looping through image:  10\n",
      "Looping through image:  11\n",
      "Looping through image:  12\n",
      "Looping through image:  13\n",
      "Looping through image:  14\n",
      "Looping through image:  15\n",
      "Looping through image:  16\n",
      "Looping through image:  17\n",
      "Looping through image:  18\n",
      "Looping through image:  19\n",
      "Looping through image:  20\n",
      "Looping through image:  21\n",
      "Looping through image:  22\n",
      "Looping through image:  23\n",
      "Looping through image:  24\n",
      "Looping through image:  25\n",
      "Looping through image:  26\n",
      "Looping through image:  27\n",
      "Looping through image:  28\n",
      "Looping through image:  29\n",
      "Looping through image:  30\n",
      "Looping through image:  31\n",
      "Looping through image:  32\n",
      "Looping through image:  33\n",
      "Looping through image:  34\n",
      "Looping through image:  35\n",
      "Looping through image:  36\n",
      "Looping through image:  37\n",
      "Looping through image:  38\n",
      "Looping through image:  39\n",
      "Looping through image:  40\n",
      "Looping through image:  41\n",
      "Looping through image:  42\n",
      "Looping through image:  43\n",
      "Looping through image:  44\n",
      "Looping through image:  45\n",
      "Looping through image:  46\n",
      "Looping through image:  47\n",
      "Looping through image:  48\n",
      "Looping through image:  49\n",
      "Looping through image:  50\n",
      "Looping through image:  51\n",
      "Looping through image:  52\n",
      "Looping through image:  53\n",
      "Looping through image:  54\n",
      "Looping through image:  55\n",
      "Looping through image:  56\n",
      "Looping through image:  57\n",
      "Looping through image:  58\n",
      "Looping through image:  59\n",
      "Looping through image:  60\n",
      "Looping through image:  61\n",
      "Looping through image:  62\n"
     ]
    }
   ],
   "source": [
    "# Get feature matches for all the images\n",
    "matches_dict = match_features(descriptors_list)\n",
    "# single_matches_dict = single_image_match_features(descriptors_list, i=0) # this function was for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 22\n",
      "1 15\n",
      "2 23\n",
      "3 21\n",
      "4 11\n",
      "5 26\n",
      "6 23\n",
      "7 24\n",
      "8 14\n",
      "9 12\n",
      "10 16\n",
      "11 5\n",
      "12 2\n",
      "13 1\n",
      "15 2\n",
      "16 21\n",
      "17 25\n",
      "18 27\n",
      "19 15\n",
      "20 3\n",
      "21 20\n",
      "22 3\n",
      "23 21\n",
      "24 24\n",
      "25 22\n",
      "26 23\n",
      "27 22\n",
      "28 15\n",
      "29 10\n",
      "30 5\n",
      "31 3\n",
      "32 6\n",
      "33 4\n",
      "34 5\n",
      "35 10\n",
      "36 5\n",
      "37 3\n",
      "38 8\n",
      "39 11\n",
      "40 4\n",
      "41 6\n",
      "42 10\n",
      "43 11\n",
      "44 16\n",
      "45 8\n",
      "46 7\n",
      "47 8\n",
      "48 10\n",
      "49 8\n",
      "50 4\n",
      "51 9\n",
      "52 19\n",
      "53 22\n",
      "55 7\n",
      "56 2\n",
      "57 1\n",
      "58 22\n",
      "59 13\n",
      "60 9\n",
      "61 10\n",
      "62 6\n",
      "The best source image is:  (18, 27)\n"
     ]
    }
   ],
   "source": [
    "# Find the best source image (where the transformation matrices will be built from)\n",
    "# Based on what fraction of the remaining images that share at least 8 good matches with the source image\n",
    "# The best % is chosen as the source image\n",
    "best_sources = {}\n",
    "for key in matches_dict:\n",
    "    source = key[0]\n",
    "    if len(matches_dict[key]) >= 8:\n",
    "        if best_sources.get(source):\n",
    "            best_sources[source] += 1\n",
    "        else:\n",
    "            best_sources[source] = 1\n",
    "\n",
    "best_source = sorted(best_sources.items(), key=lambda item: item[1])[-1]\n",
    "# for key in best_sources:\n",
    "#     print(key, best_sources[key]) # uncomment if you want to see how many values are in each set of good matches\n",
    "print(\"The best source image is: \", best_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prune images that fail to have more 8 or more good matches\n",
    "pruned_match_dict = {}\n",
    "for i in range(len(image_paths)):\n",
    "    if i == best_source[0]: continue\n",
    "    if len(matches_dict[(best_source[0], i)]) >= 8:\n",
    "        pruned_match_dict[(best_source[0], i)] = matches_dict[(best_source[0], i)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We got the focal length from the calibration for intrinsic parameters\n",
    "focal_length = 1.55197091e+03\n",
    "# We are assuming the focal point is the centre of the image - the image is 1773x1773 -> (1773/2, 1773/2) \n",
    "principal_point = (886.5, 886.5)\n",
    "# Building the intrinsic matrix for this specific camera\n",
    "K = np.array([[focal_length, 0, principal_point[0]],\n",
    "              [0, focal_length, principal_point[1]],\n",
    "              [0, 0, 1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_y_axes(T):\n",
    "    R_flip = np.array([[1, 0, 0],\n",
    "                       [0, -1, 0],\n",
    "                       [0, 0, 1]])\n",
    "    \n",
    "    # Applying the rotation to the 3x3 top-left submatrix of T\n",
    "    T[:3, :3] = T[:3, :3] @ R_flip\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_relative_pose(keypoints1, keypoints2, matches, camera_matrix):\n",
    "    if len(matches) < 8: return None, None\n",
    "    points1 = np.float32([keypoints1[m.queryIdx].pt for m in matches])\n",
    "    points2 = np.float32([keypoints2[m.trainIdx].pt for m in matches])\n",
    "\n",
    "    E, _ = cv2.findEssentialMat(points1, points2, camera_matrix, method=cv2.RANSAC, prob=0.999, threshold=1.0)\n",
    "    _, R, t, _ = cv2.recoverPose(E, points1, points2, camera_matrix)\n",
    "\n",
    "    return R, t\n",
    "\n",
    "\n",
    "# Initialize array to hold transformation matrices\n",
    "num_images = len(pruned_match_dict)\n",
    "transformation_matrices = {}\n",
    "transformation_matrices[best_source[0]] = flip_y_axes(np.identity(4))\n",
    "\n",
    "for (idx1, idx2), matches in pruned_match_dict.items():\n",
    "    R, t = estimate_relative_pose(keypoints_list[idx1], keypoints_list[idx2], matches, K)\n",
    "    if R is None: continue\n",
    "    transformation_matrix = flip_y_axes(np.eye(4))\n",
    "    transformation_matrix[:3, :3] = R\n",
    "    transformation_matrix[:3, 3] = t[:, 0]\n",
    "    transformation_matrices[idx2] = flip_y_axes(np.linalg.inv(transformation_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pruned image: 83\n",
      "pruned image: 84\n",
      "pruned image: 80\n",
      "pruned image: 87\n",
      "pruned image: 86\n",
      "pruned image: 92\n",
      "pruned image: 94\n",
      "pruned image: 103\n",
      "pruned image: 105\n",
      "pruned image: 102\n",
      "pruned image: 111\n",
      "pruned image: 112\n",
      "pruned image: 104\n",
      "pruned image: 110\n",
      "pruned image: 113\n",
      "pruned image: 107\n",
      "pruned image: 106\n",
      "pruned image: 108\n",
      "pruned image: 114\n",
      "pruned image: 117\n",
      "pruned image: 118\n",
      "pruned image: 109\n",
      "pruned image: 119\n",
      "pruned image: 115\n",
      "pruned image: 116\n",
      "pruned image: 122\n",
      "pruned image: 129\n",
      "pruned image: 120\n",
      "pruned image: 121\n",
      "pruned image: 131\n",
      "pruned image: 133\n",
      "pruned image: 134\n"
     ]
    }
   ],
   "source": [
    "# Rewrite transformation file generated by InstantNGP to test results\n",
    "file = open('assets/airpods/test/transforms.json')\n",
    "data = json.load(file)\n",
    "\n",
    "with open('assets/airpods/test/transforms4.json', 'w') as f:\n",
    "    data[\"aabb_scale\"] = 8\n",
    "    for i in range(len(data['frames'])):\n",
    "        file_path = data['frames'][i]['file_path']\n",
    "        img_index = int(file_path[4:8]) - 72\n",
    "        \n",
    "        if transformation_matrices.get(img_index) is not None:\n",
    "            data['frames'][i]['transform_matrix'] = transformation_matrices[img_index].tolist()\n",
    "        else:\n",
    "            print(\"pruned image:\", img_index + 72)\n",
    "            data['frames'][i] = None\n",
    "\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "17\n",
      "12\n",
      "22\n",
      "30\n",
      "10\n",
      "37\n",
      "86\n",
      "56\n",
      "8\n",
      "40\n",
      "10\n",
      "22\n",
      "100\n",
      "33\n",
      "23\n",
      "35\n",
      "61\n",
      "38\n",
      "46\n",
      "22\n",
      "33\n",
      "27\n",
      "8\n",
      "22\n",
      "28\n",
      "26\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# This block of code was used to visualize the quality of the matches to see the value of pruning\n",
    "def draw(img1_path, kp1, img2_path, kp2, good_matches):\n",
    "    img1 = cv2.imread(img1_path, cv2.IMREAD_GRAYSCALE)\n",
    "    img2 = cv2.imread(img2_path, cv2.IMREAD_GRAYSCALE)\n",
    "    # Draw matches\n",
    "    img_matches = cv2.drawMatches(img1, kp1, img2, kp2, good_matches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "    # Display the matches\n",
    "    cv2.namedWindow(\"Sift_Matches\", cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow(\"Sift_Matches\", 2000, 1000)\n",
    "    cv2.imshow(\"Sift_Matches\", img_matches)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Pre-pruning on best_source\n",
    "# source = best_source[0]\n",
    "# for i in range(len(image_paths)):\n",
    "#     if i == source: continue\n",
    "#     draw('assets/airpods/test/' + image_paths[source], keypoints_list[source], 'assets/airpods/test/' + image_paths[i], keypoints_list[i], matches_dict[(source, i)])\n",
    "\n",
    "# Post pruning\n",
    "print(len(pruned_match_dict))\n",
    "for (source, i), matches in pruned_match_dict.items():\n",
    "    print(len(matches))\n",
    "    draw('assets/airpods/test/' + image_paths[source], keypoints_list[source], 'assets/airpods/test/' + image_paths[i], keypoints_list[i], matches)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opencv-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
